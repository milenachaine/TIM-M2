{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92,268 texts collected.\n",
      "Training on 12,230,109 character sequences.\n",
      "WARNING:tensorflow:From /Users/duhayon-fontaine/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/duhayon-fontaine/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/1\n",
      " 5679/95547 [>.............................] - ETA: 7:42:24 - loss: 4.1189"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bf7d0303ba67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtextgenrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextgenrnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtextgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextgenrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtextgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TfIdfQuestions.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtextgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textgenrnn/textgenrnn.py\u001b[0m in \u001b[0;36mtrain_from_file\u001b[0;34m(self, file_path, header, delim, new_model, context, is_csv, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m                 texts, context_labels=context_labels, **kwargs)\n\u001b[1;32m    328\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_from_largetext_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/textgenrnn/textgenrnn.py\u001b[0m in \u001b[0;36mtrain_on_texts\u001b[0;34m(self, texts, context_labels, batch_size, num_epochs, verbose, new_model, gen_epochs, train_size, max_gen_length, validation, dropout, via_new_model, save_epochs, multi_gpu, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m                               \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                               \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                               )\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from textgenrnn import textgenrnn\n",
    "textgen = textgenrnn()\n",
    "textgen.train_from_file('TfIdfQuestions.pkl', num_epochs=1)\n",
    "textgen.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs :  6\n"
     ]
    }
   ],
   "source": [
    "def lireDocs():\n",
    "    docs = {}\n",
    "\n",
    "    fichier_tree = ET.parse(\"corpusIrisVersion4.xml\")\n",
    "    fichier_docs = fichier_tree.getroot()\n",
    "    compteur_docs = 0\n",
    "    for document in fichier_docs:\n",
    "        \n",
    "        while compteur_docs <=5:\n",
    "            compteur_docs += 1\n",
    "            doc_id = document.attrib['id']\n",
    "            questions = ''\n",
    "            answers = ''\n",
    "            doc_class = document.attrib['class']\n",
    "            doc_subclass = document.attrib['subclass']\n",
    "            for child in document:\n",
    "                if child.tag == 'question':\n",
    "                    questions += child.text.rstrip()\n",
    "\n",
    "                if child.tag == 'answer':\n",
    "                    answers += child.text.rstrip()\n",
    "\n",
    "            doc = {}\n",
    "            doc['id']= doc_id\n",
    "            doc['class'] = doc_class\n",
    "            doc['subclass'] = doc_subclass\n",
    "            doc['questions'] = questions\n",
    "            doc['answers'] = answers\n",
    "            docs[doc_id] = doc\n",
    "\n",
    "    print(\"Total docs : \", compteur_docs)\n",
    "\n",
    "    return docs\n",
    "\n",
    "docs = lireDocs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ data received. Finding features.\n",
      "Features found -- success! Calculating similarities...\n",
      "What is your question? fjeji\n",
      "Similarities found. Generating table.\n",
      "+----------------------------------------------------+----------------------+\n",
      "|                        Text                        |      Similarity      |\n",
      "+====================================================+======================+\n",
      "+----------------------------------------------------+----------------------+\n",
      "+----------------------------------------------------+----------------------+\n",
      "|                       fjeji                        |         nan          |\n",
      "+====================================================+======================+\n",
      "+----------------------------------------------------+----------------------+\n",
      "+----------------------------------------------------+----------------------+\n",
      "|                       iris1                        |         nan          |\n",
      "+====================================================+======================+\n",
      "+----------------------------------------------------+----------------------+\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "from random import sample\n",
    "import pickle\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import indicoio\n",
    "from texttable import Texttable\n",
    "indicoio.config.api_key = \"0393ee7c70f815f9ddd823d2343c2711\"\n",
    "\n",
    "'''\n",
    "Use indico's Text Features API to find text similarity and create a customer support bot that automatically responds to FAQs from users.\n",
    "Tutorial: https://indico.io/blog/faqs-bot-text-features-api/\n",
    "'''\n",
    "\n",
    "\n",
    "def make_feats(data):\n",
    "    \"\"\"\n",
    "    Send our text data through the indico API and return each text example's text vector representation\n",
    "    \"\"\"\n",
    "    chunks = [data[x:x+100] for x in range(0, len(data), 100)]\n",
    "    feats = []\n",
    "    # just a progress bar to show us how much we have left\n",
    "    for chunk in chunks:\n",
    "        feats.extend(indicoio.text_features(chunk))\n",
    "    return feats\n",
    "\n",
    "def calculate_distances(f1, f2):\n",
    "    # cosine distance is the most reasonable metric for comparison of these 300d vectors\n",
    "    distances = cdist(f1, f2, 'cosine')\n",
    "    return distances\n",
    "\n",
    "def similarity_text(idx, distance_matrix, data, n_similar=5):\n",
    "    \"\"\"\n",
    "    idx: the index of the text we're looking for similar questions to\n",
    "         (data[idx] corresponds to the actual text we care about)\n",
    "    distance_matrix: an m by n matrix that stores the distance between\n",
    "                     document m and document n at distance_matrix[m][n]\n",
    "    data: a flat list of text data\n",
    "    \"\"\"\n",
    "    t = Texttable()\n",
    "    t.set_cols_width([50, 20])\n",
    "\n",
    "    # these are the indexes of the texts that are most similar to the text at data[idx]\n",
    "    # note that this list of 10 elements contains the index of text that we're comparing things to at idx 0\n",
    "    sorted_distance_idxs = np.argsort(distance_matrix[idx])[:n_similar] # EX: [252, 102, 239, ...]\n",
    "    # this is the index of the text that is most similar to the query (index 0)\n",
    "    most_sim_idx = sorted_distance_idxs[1]\n",
    "\n",
    "    # header for texttable\n",
    "    t.add_rows([['Text', 'Similarity']])\n",
    "    print(t.draw())\n",
    "\n",
    "    # set the variable that will hold our matching FAQ\n",
    "    faq_match = None\n",
    "\n",
    "    for similar_idx in sorted_distance_idxs:\n",
    "        # actual text data for display\n",
    "        datum = data[similar_idx]\n",
    "\n",
    "        # distance in cosine space from our text example to the similar text example\n",
    "        distance = distance_matrix[idx][similar_idx]\n",
    "\n",
    "        # how similar that text data is to our input example\n",
    "        similarity =  1 - distance\n",
    "\n",
    "        # add the text + the floating point similarity value to our Texttable() object for display\n",
    "        t.add_rows([[datum, str(round(similarity, 2))]])\n",
    "        print(t.draw())\n",
    "\n",
    "        # set a confidence threshold\n",
    "        # TODO\n",
    "\n",
    "    # print the appropriate answer to the FAQ, or bring in a human to respond\n",
    "    # TODO\n",
    "\n",
    "def input_question(data, feats):\n",
    "    # input a question\n",
    "    question = input(\"What is your question? \")\n",
    "    # add the user question and its vector representations to the corresponding lists, `data` and `feats`\n",
    "    # insert them at index 0 so you know exactly where they are for later distance calculations\n",
    "    if question is not None:\n",
    "        data.insert(0, question)\n",
    "    new_feats = indicoio.text_features(question)\n",
    "    feats.insert(0, new_feats)\n",
    "    return data, feats\n",
    "\n",
    "def run():\n",
    "    data = list(docs.keys())\n",
    "    print(\"FAQ data received. Finding features.\")\n",
    "\n",
    "    feats = make_feats(data)\n",
    "    with open('faq_feats.pkl', 'wb') as f:\n",
    "        pickle.dump(feats, f)\n",
    "\n",
    "    with open('faq_feats.pkl', 'rb') as f:\n",
    "        feats = pickle.load(f)\n",
    "        #pickle.dump(feats, f)\n",
    "\n",
    "    print(\"Features found -- success! Calculating similarities...\")\n",
    "    #print(\"FAQ features found!\")\n",
    "\n",
    "    new_data, new_feats = input_question(data, feats)\n",
    "\n",
    "    distance_matrix = calculate_distances(feats, new_feats)\n",
    "    print(\"Similarities found. Generating table.\")\n",
    "\n",
    "    idx = 0\n",
    "    similarity_text(idx, distance_matrix, new_data)\n",
    "    print('\\n' + '-' * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Je pense que vous avez donné la réponse dans votre premier message ', \" Les textes sont souvent volontairement flous pour laisser la souveraineté d' appréciation aux juges , chaque juge ayant la sienne \", \" Il n' y aurait donc pas de réponse formelle à votre question \", ' Bonjour , je viens quérir auprès de vous des conseils et surtout des éclaircissements relatifs au changement de nom pour le motif de \" relever un nom qui s\\' est illustré sur le plan national \" ( art ', ' 61 du Code Civil ) ', \" La notion me semble relativement floue , et laisse aux magistrats la possibilité d' apprécier très largement cette notion \", \" Outre le peu d' exemples que l' on trouve ici et là ( de Talleyrand , de Tocqueville , etc \", \" ) , en existe-t-il d' autres ? Savez -vous si les magistrats estiment par exemple que les personnages politiques ( députés , sénateurs , ministres ) son susceptibles d' entrer dans ce champ ? Y a-t-il des précédents ? Merci d' avance pour vos lumières Pour ce qui est de la décision à laquelle je faisais référence , il s' agissait de la n ° 28072 du Conseil d' Etat en 2006 , et non la Cour de Cassation en 2013 comme je l' avais écrit \", ' Pour répondre à Bonnevolonté , il est précisé qu\\' il est possible de faire la demande du changement de nom \" si vous portez le nom d\\' une personne célèbre avec une mauvaise réputation \" Je ne désespère pas qu\\' une bonne âme puisse m\\' éclairer sur cette notion d\\' illustration PS ', \" J' aurai bien inséré des liens mais il semble que je n' y sois pas autorisé \", \" Tout d' abord merci pour vos réponses \", \" La demande pour relever un nom « illustre » n' est pas subordonnée au fait d' etre un descendant direct ou collatéral jusqu' au 4ème degré \", \" Une décision de la Cour de Cassation l' a rappelé en 2013\", ' Preuve en est la démarche de M ', ' Parisot qui a relevé le nom de son aïeul collatéral le chevalier de Bayard qui vécut au XVe siècle ', \" Ma demande ne porte pas sur l' opportunité de relever le nom d' un de mes ancêtres , mais de la définition que peuvent faire les magistrats de la notion de « nom qui s' est illustré sur le plan national » \", \" Bonjour , Modérateur 01 désolé pour l' oubli \", ' Cordialement Bonjour ', \" Je suis nouveau ici , le peu que j' ai lu me plait beaucoup et je suis certain que certain d' entre vous m' aideront à trouver des réponses \", ' Voilà ', \" J' ai 35 ans , mes parents sont en vie mais divorcés , je suis allé cherché aujourd'hui ma nouvelle CNI sur laquelle figure à ma demande un nom d' usage ( le nom de jeune fille de ma maman ) \", \" Questions : - Quand puis -je l' utiliser ( administrations , Banque etc ) pour que ce nom figure sur les différents documents et courriers ? - vient le moment de la signature \", '', '', \" Ai -je le droit de signer mon nom d' usage simplement ou doit-il figurer en plus de mon nom de naissance ( nom d' usage+nom de naissance ) ? D' avance merci pour vos réponses \", ' Bonjour ', \" Vu que je suis nouveau , je me présente rapidement : Rémy , étudiant en master à l' ESPE ( afin de devenir professeur des écoles ) \", \" Je me pose une question juridique à propos du droit à l' image ou à la personne concernant les enfants d' une classe de primaire \", ' J\\' aimerai faire comme projet des petits ateliers philosophiques qui permettra aux enfants à développer le domaine du langage et leurs capacités à raisonner ( par exemple , en posant la question : \" est -ce bien d\\' être différent \" ) ', \" Et ces ateliers j' aimerais : à la fois les diffuser sur ( une plateforme de vidéos en ligne ) en direct ( pour la transparence avec les parents et pour partager avec d' autres professeurs ou étudiants ) , les laisser en rediffusion avec des sous-titres , et enfin , mettre aussi sur le site de l' école avec un format texte \", ' Sur ce format texte , je pense que ce serait intéressant de mettre les prénoms afin de faciliter la compréhension du lecteur ', \" Ainsi , je sais que je dois avoir des autorisations des parents pour pouvoir diffuser sur le compte ( une plateforme de vidéos en ligne ) de la classe pour une durée d' un an par exemple \", \" Mais je ne sais pas quelles sont les autorisations à avoir pour mettre le document texte sur le site de l' école \", \" Et si ces autorisations sont temporaires , aurais -je le droit de modifier les noms à la fin du délai d' autorisation et laisser les documents sur le site de l' école , ou devrais -je définitivement tout supprimer ? Je vous remercie de m' avoir lu et de vos éventuelles réponses \", \" Je vous remercie , mais si j' enlève les noms , ça va être très compliqué pour lecteurs de comprendre et en plus les enfants ne risquent de ne pas se sentir valorisés de la même manière \", \" Je n' aurais vraiment pas le droit de laisser ces noms même avec une autorisation ? Et quand je n' ai plus l' autorisation , j' ai le droit de remplacer les noms par des pseudos mais laisser les textes écrits en ligne ? Merci en tous cas de la réponse \", \" A la rigueur , chaque enfant peut choisir son pseudo dès le début c' est vrai \", ' Merci et désolé pour cette erreur ', \" Vu que la plateforme vidéo que j' avais choisi était une entreprise vu que l' on parle souvent de vidéo ------- \", ' Bonjour , ahah ! j\\' adore le commentaire de \" bien luné aujourd\\'hui \" bonne chance à remyespe dans son projet signé : curieuse Bonjour , merci beaucoup pour votre aide ! bonjour , le principe est l\\' immutabilité de son nom de famille , comme vous êtes née avant 2005 , vous ne pouviez recevoir que le nom de votre père comme nom de famille ', ' vous pouvez demander un changement de nom de famille pour motif légitime ', \" voir ce lien : url-remplacée vous pouvez vous renseigner auprès du service d' état-civil de votre commune \", \" salutations Bonjour , Je suis majeure depuis quelques mois et j' aimerais obtenir un double nom , à savoir ajouter celui de ma mère à la suite de celui de mon père \", \" Est -ce possible de le faire ajouter directement à l' état civil et pas seulement en nom d' usage ? Je tiens vraiment à récupérer ce nom de famille car je suis la seule enfant de la famille et je ne voudrais pas qu' il disparaisse \", \" A la limite s' il n' est pas possible de l' accoler à celui de mon père sur l' état civil , est-il possible de carrément l' échanger ? Je suis née en Novembre 1999 et mes parents n' ont jamais été mariés \", \" J' ai déjà lu à peu près tout et son contraire sur internet mais les lois changent et évoluent donc je suis un peu perdue et je ne connais pas les procédures nécessaires à ce changement de nom \", \" Merci beaucoup ! ! bonjour J' ai demandé a faire retirer le prélèvement automatique pour ma fille ( je passe sur les détails ) J' ai du prendre un avocat \", ' Mon ex femme travaille au ministère de la justice dans un TGI comme greffière ', \" Je suis pour le moins étonné qu' elle communique ses conclusions et pièces jointes à mon avocat par sa messagerie du ministère de la justice \", \" J ( elle n' a pas pris d' avocat ) Je suis dans le domaine informatique et je sais très bien que ce type de messagerie étant administrée en interne , elle peut être visible facilement par des tiers tels que les administrateurs des serveurs de messagerie \", ' Je trouve cette démarche anormale ', \" L' utilisation de cette messagerie permet entre autre de pouvoir communiquer avec d' autres personnes du ministère de la justice \", '', '', ' ', ' Je préfère ne pas développer cet aspect ', ' Souvent les messageries extérieures sont bloquées ', \" Qu' en pensez_Vous ? ? merci bonjour j' ne pense que ces messageries sont infiniment plus sécurisées que celles accessibles sur le net \", '', '', ' ', ' ', \" et que les administrateurs messageries ( surtout au niveau d' un ministère ) ont bien d' autres choses à faire que de lire les messages envoyés par leurs collègues \", '', '', ' ', ' et pensez vous que les autres types de messageries ne sont pas lisibles par \" leurs \" administrateurs ? en plus du risque de \" piratage \" ', '', '', ' ', ' ', \" Bonjour à tous , Comme la loi m' y autorise , je souhaiterai ajouter le nom de jeune fille de ma mère à mon nom patronymique et créer ainsi un double-nom d' usage ( Dupont Durand ) \", ' Sur le site ANTS , la zone \" autre nom \" n\\' est pas claire ', ' Est -ce que je dois y ajouter seulement le nom de ma mère , ou le double-nom que je souhaite créer ? Ainsi , si je m\\' appelle actuellement Dupont , il sera indiqué sur mon passeport \" Dupont usage Dupont-Durand \" ? Merci d\\' avance pour vos réponses ', \" Bonjour , Lien vers un site officiel url-remplacée On peut écrire ce qu' on veut sur un forum ; personne n' est obligé de nous croire \", '']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# le chemin vers tous les fichiers conll\n",
    "\n",
    "folder_path = glob.glob(\"corpus-test/*/*.conll\")\n",
    "\n",
    "id_fi = []\n",
    "lists = []\n",
    "\n",
    "\n",
    "pat = re.compile(\"iris[0-9]{3,5}_a.+\\.conll$\")\n",
    "pat2 = re.compile(\"[a-z]\\'[a-z]+\")\n",
    "\n",
    "# on parcourt la liste des chemins des fichiers conll\n",
    "for p in folder_path:\n",
    "\n",
    "    # l'identifiant d'un fichier correpond à cette regex\n",
    "    filename = pat.search(p)\n",
    "\n",
    "    # on récupère chaque id et on le met dans une liste \n",
    "    id_fi.append(filename)\n",
    "\n",
    "    # pour chaque fichier lu\n",
    "\n",
    "    with open(p, 'r') as files:\n",
    "        # à chaque passage d'un fichier lu\n",
    "        lists_fi = []\n",
    "        for file in files:\n",
    "            cols = file.rstrip('\\n').split('\\t')\n",
    "            if len(cols) == 3:\n",
    "                # on ajoute la colonnes des lemmes dans une liste\n",
    "                lists_fi.append(cols[0])\n",
    "    # puis, on met toutes les listes des lemmes dans une liste globale\n",
    "    lists.append(lists_fi)\n",
    "    \n",
    "#print(lists)\n",
    "\n",
    "file = open(\"text.txt\", \"w\")\n",
    "\n",
    "phrase = \"\"\n",
    "for element in lists:\n",
    "    for mot in element:\n",
    "        phrase = phrase +\" \"+ mot\n",
    "    file.write(phrase)\n",
    "\n",
    "file.close()\n",
    "        \n",
    "        \n",
    "conversation = phrase.split(\".\")\n",
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/duhayon-fontaine/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /Users/duhayon-\n",
      "[nltk_data]     fontaine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "List Trainer: [####################] 100%\n"
     ]
    }
   ],
   "source": [
    "from chatterbot import ChatBot\n",
    "chatbot = ChatBot(\"Ron Obvious\")\n",
    "\n",
    "from chatterbot.trainers import ListTrainer\n",
    "\n",
    "trainer = ListTrainer(chatbot)\n",
    "\n",
    "trainer.train(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vu que je suis nouveau , je me présente rapidement : Rémy , étudiant en master à l' ESPE ( afin de devenir professeur des écoles )\n"
     ]
    }
   ],
   "source": [
    "response = chatbot.get_response(\"Mes parents sont divorcés, puis-je le changer? \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('___BEGIN__', '___BEGIN__')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-217-8fc4ea54338d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Build the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarkovify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Print five randomly-generated sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/markovify/text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_text, state_size, chain, parsed_sentences, retain_original)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m# Rejoined text lets us assess the novelty of generated sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrejoined_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_join\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_join\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchain\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsed_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/markovify/chain.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, state_size, model)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecompute_begin_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/markovify/chain.py\u001b[0m in \u001b[0;36mprecompute_begin_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \"\"\"\n\u001b[1;32m     79\u001b[0m         \u001b[0mbegin_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0mBEGIN\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mchoices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbegin_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mcumdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_cumdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcumdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('___BEGIN__', '___BEGIN__')"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "\n",
    "# Get raw text as string.\n",
    "with open(\"text.txt\", \"rb\") as f:\n",
    "    text = f.read().decode(\"UTF-8\")\n",
    "    #print(text)\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "# Print five randomly-generated sentences\n",
    "for i in range(5):\n",
    "    print(text_model.make_sentence())\n",
    "\n",
    "# Print three randomly-generated sentences of no more than 140 characters\n",
    "for i in range(3):\n",
    "    print(text_model.make_short_sentence(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n",
      "\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python3\n",
    "# -*- coding: utf-8 -*\n",
    "\n",
    "# premier test parcours\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# le chemin vers tous les fichiers conll\n",
    "\n",
    "folder_path = glob.glob(\"/Users/ferialyahiaoui/Documents/cours/TIM-M2/Groupes/Pretraitement/Corpus/net-iris/*/*/*/*.conll\")\n",
    "\n",
    "#folder_path = glob.glob(\"/Users/ferialyahiaoui/Documents/cours/*.conll\")\n",
    "\n",
    "\n",
    "\n",
    "id_fi = []\n",
    "lists = []\n",
    "dico_q = {} # dico des questions : clé : id_fichier, valeur : liste des lemmes\n",
    "\n",
    "\n",
    "#pat1 = re.compile('iris([0-9]{3,5})_q([0-9]{1,3})\\.conll$')\n",
    "\n",
    "pat = re.compile(\"iris[0-9]{3,5}_[q|a].+\\.conll$\")\n",
    "\n",
    "\n",
    "\n",
    "# on parcourt la liste des chemins des fichiers conll\n",
    "for p in folder_path:\n",
    "\n",
    "\t# l'identifiant d'un fichier correpond à cette regex\n",
    "\tfilename = pat.search(p)\n",
    "\tif filename:\n",
    "\t\tid_fi.append(filename.group())\n",
    "\t\t#print(filename.group())\n",
    "#print(id_fi)\n",
    "\n",
    "\t# on récupère chaque id et on le met dans une liste \n",
    "\t\n",
    "\t# pour chaque fichier lu\n",
    "\t\n",
    "\twith open(p, 'r') as files:\n",
    "\t\t# à chaque passage d'un fichier lu\n",
    "\t\tlists_w = []\n",
    "\n",
    "\t\t\n",
    "\t\tfor file in files:\n",
    "\t\t\tcols = file.rstrip('\\n').split('\\t')\n",
    "\t\t\tif len(cols) == 3:\n",
    "\t\t\t\t# on ajoute la colonnes des lemmes dans une liste\n",
    "\t\t\t\tlists_w.append(cols[2])\n",
    "\t\t#print(lists_w)\n",
    "\t\t# on parcourt la liste des id des fichiers \n",
    "\t\tfor filename in id_fi:\n",
    "\t\t\t# on crée un dic avec comme clé : id_fic et comme valeur : liste de lemmes du fic en question\n",
    "\t\t\tdico_q[filename] = lists_w\n",
    "\n",
    "\t# puis, on met toutes les listes des lemmes dans une liste globale\n",
    "\tlists.append(lists_w)\n",
    "print(lists)\n",
    "print('\\n')\n",
    "print(dico_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
