{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xirong/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus and finding features\n"
     ]
    }
   ],
   "source": [
    "print('Reading corpus and finding features')\n",
    "xmlcorpus = ET.parse('../Corpus/all.xml')\n",
    "textes = xmlcorpus.findall('.//text')\n",
    "corpus = []\n",
    "for i in textes:\n",
    "    corpus.append(i.text)\n",
    "# les étiquettes du document (références)\n",
    "docs= xmlcorpus.getroot().getchildren()\n",
    "Y = np.zeros((len(docs), 1))\n",
    "for i in range(len(docs)):\n",
    "    doc = docs[i]\n",
    "    fake = 0\n",
    "    if doc.get('class') == 'fake':\n",
    "        fake = 1\n",
    "    Y[i,0] = fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "transformer = TfidfTransformer()\n",
    "counts = vectorizer.fit_transform(corpus)\n",
    "tfidf = transformer.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [3, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 3, 0]], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.04821643, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.09863567,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidf.toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# définir la taille du batch de donnée d'entraînment\n",
    "batch_size = 5\n",
    "dataset_size = X.shape[0]\n",
    "lenvec = X.shape[1]\n",
    "# définir les paramètres de réseau de neurones\n",
    "w1 = tf.Variable(tf.random_normal([lenvec,3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3,1], stddev=1, seed=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 12070)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "分数据集（训练集，测试集 ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "在shape的一个维度上使用None可以方便使用不大的batch大小。(None: 不定)\n",
    "\"\"\"\n",
    "x = tf.placeholder(tf.float32, shape=(None, lenvec), name = 'x-input')\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1), name = 'y-input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义神经网络前向传播过程\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 定义损失函数来刻画预测值和真实值的差距\n",
    "# y_代表正确结果，y代表预测结果\n",
    "# clip_by_value函数可以吧张量中的数值限定在一个范围之中， 这样可以避免一些运算错误（比如log0是无效的）\n",
    "# tf.clip_by_value(y, 1e-10, 1.0) 就是把y值限定在1e-10与1.0之间\n",
    "# reduce_mean()对整个矩阵求平均， 用于得到一个batch的平均交叉熵\n",
    "cross_entropy = -tf.reduce_mean(\n",
    "    y_*tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n",
    "# 定义学习率\n",
    "learning_rate = 0.001\n",
    "# 定义反向传播算法来优化神经网络中的参数\n",
    "algo_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练之前神经网络参数的值\n",
      "[[-0.8113182   1.4845988   0.06532937]\n",
      " [-2.4427042   0.0992484   0.5912243 ]\n",
      " [ 0.59282297 -2.1229296  -0.72289723]\n",
      " ...\n",
      " [ 1.3733915   0.13433133 -1.415918  ]\n",
      " [-0.20769614  1.2466617   1.8288933 ]\n",
      " [-0.41405147  0.6377421  -0.9574355 ]]\n",
      "[[-0.8113182 ]\n",
      " [ 1.4845988 ]\n",
      " [ 0.06532937]]\n",
      "After 0 training step(s), cross entropy on all data is 5.548383712768555\n",
      "After 1000 training step(s), cross entropy on all data is 2.9308037757873535\n",
      "After 2000 training step(s), cross entropy on all data is 1.5655864477157593\n",
      "After 3000 training step(s), cross entropy on all data is 1.5613031387329102\n",
      "After 4000 training step(s), cross entropy on all data is 1.5610747337341309\n",
      "训练后的神经网络参数的值\n",
      "[[-0.86080456  1.5360957   0.09585378]\n",
      " [-2.4427042   0.0992484   0.5912243 ]\n",
      " [ 0.5833129  -2.11339    -0.7137513 ]\n",
      " ...\n",
      " [ 1.3425592   0.16571797 -1.3868986 ]\n",
      " [-0.26227173  1.3022805   1.849376  ]\n",
      " [-0.41405147  0.6377421  -0.9574355 ]]\n",
      "[[-0.71392375]\n",
      " [ 1.6129307 ]\n",
      " [-0.04042413]]\n"
     ]
    }
   ],
   "source": [
    "# 创建一个会话来运行tf程序\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    # 初始化变量\n",
    "    sess.run(init_op)\n",
    "    # 在训练之前神经网络参数的值\n",
    "    print(\"训练之前神经网络参数的值\")\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))\n",
    "        # 设定训练的轮数\n",
    "    steps = 5000\n",
    "    for i in range(steps):\n",
    "        # 每次选取batch_size个样本进行训练\n",
    "        start = (i*batch_size)%dataset_size\n",
    "        end = min(start+batch_size, dataset_size)\n",
    "\n",
    "        # 通过选取的样本训练神经网络并更新参数\n",
    "        sess.run(algo_op, feed_dict = {x: X[start:end], y_:Y[start:end]})\n",
    "        if i%1000 == 0:\n",
    "            # 每隔一段时间计算在所有数据上的交叉熵并输出\n",
    "            total_cross_entropy = sess.run(cross_entropy, feed_dict = {x: X, y_: Y})\n",
    "            print(\"After {} training step(s), cross entropy on all data is {}\".format(i, total_cross_entropy))\n",
    "    print(\"训练后的神经网络参数的值\")\n",
    "    print(sess.run(w1))\n",
    "    print(sess.run(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
